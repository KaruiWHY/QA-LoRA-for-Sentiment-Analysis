# -*- coding: utf-8 -*-
"""
lora_w4a8_qalora.py
----------------------
W4A8 å…¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Full QAT) + QA-LoRA
- Stage 1: FP16 LoRA Warmup
- Stage 2: W4A8 QA-LoRA (Weight INT4 + Activation INT8 + Group-wise Adaptation)

é€‚é… Qwen-1.8B ç­‰æ¨¡å‹ï¼Œæ”¯æŒ AMD ROCm / CUDAã€‚
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import random, argparse, json
import matplotlib.pyplot as plt
import numpy as np
from typing import List, Tuple
from sklearn.metrics import precision_recall_fscore_support
from datasets import load_from_disk, load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    TrainerCallback,
)

from peft import LoraConfig, get_peft_model, PeftModel
from peft.tuners.lora.layer import Linear as LoRALinear
import gc


# ======================================================
# 1. æ ¸å¿ƒé‡åŒ–ç»„ä»¶ (W4A8 + STE)
# ======================================================

class FakeQuantSTE(torch.autograd.Function):
    """
    Straight-Through Estimator (STE) for INT8 Activation Quantization
    """

    @staticmethod
    def forward(ctx, x, scale):
        scale = torch.clamp(scale, min=1e-8)
        # é‡åŒ–åˆ° INT8 (-127, 127)
        q = torch.clamp(torch.round(x / scale), -127, 127)
        return q * scale

    @staticmethod
    def backward(ctx, grad_output):
        # ç›´é€šä¼°è®¡ï¼šæ¢¯åº¦ç›´æ¥ç©¿é€ï¼Œå¿½ç•¥ Round æ“ä½œçš„ä¸å¯å¯¼æ€§
        return grad_output, None


def calc_scale_tensorwise(x, eps=1e-8):
    """è®¡ç®—æ¿€æ´»å€¼çš„åŠ¨æ€é‡åŒ– Scale (Tensor-wise)"""
    max_abs = x.detach().abs().amax()
    return torch.clamp(max_abs / 127.0, min=eps)

def calc_scale_tokenwise(x, eps=1e-8):
    """
    [æ”¹è¿›] è®¡ç®—æ¿€æ´»å€¼çš„åŠ¨æ€é‡åŒ– Scale (Token-wise / Per-Row)
    
    Args:
        x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶é€šå¸¸ä¸º [Batch, Seq, Dim]
    Returns:
        scale: å½¢çŠ¶ä¸º [Batch, Seq, 1] çš„å¼ é‡ï¼Œä¿æŒç»´åº¦ä»¥ä¾¿å¹¿æ’­
    """
    # dim=-1 è¡¨ç¤ºåœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆç‰¹å¾ç»´åº¦ï¼‰ä¸Šæ‰¾æœ€å¤§å€¼
    # keepdim=True ä¿æŒç»´åº¦ï¼Œç»“æœå½¢çŠ¶ä» [B, S, D] å˜ä¸º [B, S, 1]
    max_abs = x.detach().abs().amax(dim=-1, keepdim=True)
    
    return torch.clamp(max_abs / 127.0, min=eps)


class QALoRALayer(nn.Module):
    def __init__(self, base_layer: nn.Module, r: int = 8, lora_alpha: int = 16, group_size: int = 32):
        super().__init__()
        self.in_features = base_layer.in_features
        self.out_features = base_layer.out_features
        self.group_size = group_size

        if self.in_features % group_size != 0:
            raise ValueError(f"in_features ({self.in_features}) must be divisible by group_size ({group_size})")

        self.weight = base_layer.weight
        self.weight.requires_grad = False

        self.r = r
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / r
        self.reduced_dim = self.in_features // group_size

        # LoRA å‚æ•°
        self.lora_A = nn.Parameter(torch.zeros(r, self.reduced_dim))
        self.lora_B = nn.Parameter(torch.zeros(self.out_features, r))

        nn.init.kaiming_uniform_(self.lora_A, a=5 ** 0.5)
        nn.init.zeros_(self.lora_B)
        
        # [æ–°å¢] ç”¨äºå­˜å‚¨é‡åŒ–å‚æ•°ï¼Œä»¥ä¾¿åˆå¹¶æ—¶ä½¿ç”¨ (è®­ç»ƒæ—¶åŠ¨æ€è®¡ç®—ï¼Œæ¨ç†/åˆå¹¶æ—¶å›ºå®š)
        self.register_buffer('quant_scale', None)
        self.register_buffer('quant_zero', None)

    def fake_quant_activation(self, x):
        """ä¿®æ”¹ä¸º Token-wise é‡åŒ–"""
        scale = calc_scale_tokenwise(x)
        return FakeQuantSTE.apply(x, scale)

    def fake_quant_weight_asym(self, w):
        """
        [ä¿®æ”¹é‡ç‚¹] è®ºæ–‡å…¬å¼ (1): Group-wise Asymmetric Min-Max Quantization
        w shape: [Out, In]
        """
        out_dim, in_dim = w.shape
        
        # 1. Reshape to [Out, Num_Groups, Group_Size]
        w_reshaped = w.reshape(out_dim, in_dim // self.group_size, self.group_size)
        
        # 2. è®¡ç®— Min/Max (Group-wise)
        # shape: [Out, Num_Groups, 1]
        max_val = w_reshaped.amax(dim=-1, keepdim=True)
        min_val = w_reshaped.amin(dim=-1, keepdim=True)
        
        # 3. è®¡ç®— Alpha (Scale) å’Œ Beta (Zero Point)
        # INT4 range: 0 to 15 (2^4 - 1)
        # é¿å…é™¤ä»¥0ï¼ŒåŠ ä¸Š eps
        alpha = (max_val - min_val) / 15.0
        alpha = torch.clamp(alpha, min=1e-5)
        beta = min_val
        
        # 4. Quantize (å…¬å¼ 1)
        # W_int = Round((W - Beta) / Alpha)
        w_int = ((w_reshaped - beta) / alpha).round().clamp(0, 15)
        
        # 5. Dequantize (ç”¨äºå‰å‘ä¼ æ’­)
        # W_recon = W_int * Alpha + Beta
        w_recon = w_int * alpha + beta
        
        # Reshape å›åŸå§‹å½¢çŠ¶
        w_recon = w_recon.reshape(out_dim, in_dim)
        
        # [å¯é€‰] ä¿å­˜å½“å‰çš„ç»Ÿè®¡æ•°æ®ä»¥ä¾¿åç»­åˆ†ææˆ–åˆå¹¶
        if self.training:
            self.quant_scale = alpha.detach() # [Out, Groups, 1]
            self.quant_zero = beta.detach()   # [Out, Groups, 1]
            
        return w_recon, alpha, beta

    def forward(self, x):
        # 1. Activation Quantization (A8)
        x_q = self.fake_quant_activation(x)

        # 2. Weight Quantization (W4 Asymmetric) [ä¿®æ”¹ç‚¹]
        w_q, _, _ = self.fake_quant_weight_asym(self.weight)

        # 3. Base Computation
        base_out = F.linear(x_q, w_q)

        # 4. QA-LoRA Path (ä¿æŒä¸å˜)
        b, s, d = x_q.shape
        x_reshaped = x_q.reshape(b, s, d // self.group_size, self.group_size)
        x_pooled = x_reshaped.sum(dim=-1) # Group-wise Sum

        lora_input = x_pooled.to(self.lora_A.dtype)
        lora_out = (lora_input @ self.lora_A.T @ self.lora_B.T) * self.scaling

        return base_out + lora_out.to(base_out.dtype)

    def merge(self):
        """
        [æ–°å¢åŠŸèƒ½] å®ç°è®ºæ–‡ Appendix B.3 çš„æ— æŸåˆå¹¶é€»è¾‘
        æ— éœ€é‡æ–°é‡åŒ–æƒé‡ï¼Œåªéœ€æ›´æ–° Zero Point (Beta)ã€‚
        """
        with torch.no_grad():
            out_dim, in_dim = self.weight.shape
            
            # 1. é‡æ–°è·å–å½“å‰çš„é‡åŒ–å‚æ•° (Alpha, Beta)
            w_reshaped = self.weight.reshape(out_dim, in_dim // self.group_size, self.group_size)
            max_val = w_reshaped.amax(dim=-1, keepdim=True)
            min_val = w_reshaped.amin(dim=-1, keepdim=True)
            alpha = (max_val - min_val) / 15.0
            alpha = torch.clamp(alpha, min=1e-5)
            beta = min_val # åŸå§‹ Zero Point
            
            # 2. è®¡ç®— LoRA çš„è´¡çŒ®
            # LoRA å®é™…ä¸Šæ˜¯åœ¨æ¯ä¸ª Group ä¸ŠåŠ äº†ä¸€ä¸ª Bias
            # shape: [Out, r] @ [r, Num_Groups] -> [Out, Num_Groups]
            lora_delta = (self.lora_B @ self.lora_A) * self.scaling
            
            # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é… Beta [Out, Num_Groups, 1]
            lora_delta = lora_delta.unsqueeze(-1)
            
            # 3. åˆå¹¶åˆ° Zero Point
            # è®ºæ–‡å…¬å¼ (7) çš„æ¨å¯¼é€»è¾‘ï¼šæ–°çš„ Zero Point = æ—§ Zero Point + LoRAéƒ¨åˆ†
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åšçš„æ˜¯åŠ æ³•ï¼Œå› ä¸º W_recon = Q*alpha + beta + LoRA
            # => W_recon = Q*alpha + (beta + LoRA)
            new_beta = beta + lora_delta
            
            # 4. è·å– INT4 æƒé‡
            w_int = ((w_reshaped - beta) / alpha).round().clamp(0, 15)
            
            return w_int, alpha, new_beta


def convert_to_qalora_w4a8(model, r=8, lora_alpha=16, group_size=32, target_modules=["q_proj", "v_proj"]):
    """
    å°†æ¨¡å‹è½¬æ¢ä¸º W4A8 QA-LoRA æ¶æ„
    """
    print(f"ğŸ”„ æ­£åœ¨å°†æ¨¡å‹è½¬æ¢ä¸º W4A8 QA-LoRA (Group Size={group_size})...")

    def replace_module(module, current_name=""):
        for name, child in module.named_children():
            full_name = f"{current_name}.{name}" if current_name else name

            # æ‰¾åˆ°ç›®æ ‡ Linear å±‚ (å¯èƒ½æ˜¯ Peft çš„ LoRALinear æˆ– æ™®é€š Linear)
            # æˆ‘ä»¬åªæ›¿æ¢åœ¨ target_modules é‡Œçš„å±‚
            if isinstance(child, (LoRALinear, nn.Linear)):
                # åˆ¤æ–­åå­—åŒ¹é…
                is_target = any(t in name for t in target_modules)  # ç®€å•åŒ¹é… key

                # å¦‚æœæ˜¯ Peft LoRALinearï¼Œæˆ‘ä»¬éœ€è¦æå– base_layer
                if isinstance(child, LoRALinear) and is_target:
                    print(f"  -> æ›¿æ¢å±‚ (Peft): {full_name}")
                    base_layer = child.base_layer
                    new_layer = QALoRALayer(base_layer, r=r, lora_alpha=lora_alpha, group_size=group_size)
                    
                    # å…ˆè½¬åˆ°ç›®æ ‡è®¾å¤‡å’Œ dtype (FP16)
                    new_layer = new_layer.to(base_layer.weight.device).to(base_layer.weight.dtype)
                    
                    # å…³é”®ä¿®æ­£ï¼šå¼ºåˆ¶å°†å¯è®­ç»ƒå‚æ•°è½¬å› FP32
                    new_layer.lora_A.data = new_layer.lora_A.data.float()
                    new_layer.lora_B.data = new_layer.lora_B.data.float()

                    setattr(module, name, new_layer)

                # å¦‚æœå·²ç»æ˜¯æ™®é€š Linear (Stage 2 é‡æ–°åŠ è½½ raw model æ—¶)
                elif isinstance(child, nn.Linear) and is_target:
                    print(f"  -> æ›¿æ¢å±‚ (Linear): {full_name}")
                    new_layer = QALoRALayer(child, r=r, lora_alpha=lora_alpha, group_size=group_size)
                    
                    # å…ˆè½¬åˆ°ç›®æ ‡è®¾å¤‡å’Œ dtype (FP16)
                    new_layer = new_layer.to(child.weight.device).to(child.weight.dtype)
                    
                    # å…³é”®ä¿®æ­£ï¼šå¼ºåˆ¶å°†å¯è®­ç»ƒå‚æ•°è½¬å› FP32
                    new_layer.lora_A.data = new_layer.lora_A.data.float()
                    new_layer.lora_B.data = new_layer.lora_B.data.float()
                    
                    setattr(module, name, new_layer)
            else:
                replace_module(child, full_name)

    replace_module(model)
    return model


def save_merged_model(model, output_dir):
    """
    [æ–°å¢åŠŸèƒ½] å®ç° QA-LoRA æ— æŸåˆå¹¶å¹¶ä¿å­˜
    """
    print(">>> æ­£åœ¨è¿›è¡Œ QA-LoRA æ— æŸåˆå¹¶...")
    os.makedirs(output_dir, exist_ok=True)
    
    quantized_state = {}
    
    for name, module in model.named_modules():
        if isinstance(module, QALoRALayer):
            # è·å–åˆå¹¶åçš„å‚æ•°
            w_int, alpha, new_beta = module.merge()
            
            # ä¿å­˜åˆ°å­—å…¸ (æ¨¡æ‹Ÿä¿å­˜ä¸ºé‡åŒ–æ¨¡å‹æ ¼å¼ï¼Œå¦‚ GPTQ/AWQ æ ¼å¼)
            quantized_state[f"{name}.w_int"] = w_int.cpu()      # INT4 æƒé‡
            quantized_state[f"{name}.scale"] = alpha.cpu()      # FP16 Scale
            quantized_state[f"{name}.zero"] = new_beta.cpu()    # FP16 Zero Point (å·²èåˆ LoRA)
            
    torch.save(quantized_state, f"{output_dir}/merged_qalora_w4.pt")
    print(f"åˆå¹¶å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {output_dir}")


# ======================================================
# 2. è¾…åŠ©å·¥å…·
# ======================================================

def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


class TrainingMetricsCallback(TrainerCallback):
    def __init__(self):
        super().__init__()
        self.metrics = {'step': [], 'loss': [], 'learning_rate': []}

    def on_step_end(self, args, state, control, **kwargs):
        if state.log_history and state.log_history[-1].get('loss') is not None:
            log = state.log_history[-1]
            self.metrics['step'].append(state.global_step)
            self.metrics['loss'].append(log['loss'])
            self.metrics['learning_rate'].append(log.get('learning_rate', 0.0))

            # [æ–°å¢] å®æ—¶æ‰“å°æ˜¾å­˜å ç”¨
            if torch.cuda.is_available():
                mem_used = torch.cuda.max_memory_allocated() / 1024**3
                # æ‰“å°åˆ°æ§åˆ¶å°ï¼Œä½¿ç”¨ \r è¦†ç›–å½“å‰è¡Œï¼Œé¿å…åˆ·å±å¤ªå¿«
                # æ³¨æ„ï¼šTrainer è‡ªèº«çš„è¿›åº¦æ¡å¯èƒ½ä¼šè¦†ç›–è¿™ä¸ªè¾“å‡ºï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥é€‰æ‹©æ¯éš” N æ­¥æ‰“å°ä¸€æ¬¡
                if state.global_step % 10 == 0:
                    print(f" [Step {state.global_step}] Loss: {log['loss']:.4f} | Max Mem: {mem_used:.2f} GB")
                    # é‡ç½®å³°å€¼ï¼Œä»¥ä¾¿è§‚å¯Ÿä¸‹ä¸€ä¸ªåŒºé—´çš„å³°å€¼
                    torch.cuda.reset_peak_memory_stats()

    def save_metrics(self, save_path):
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, ensure_ascii=False, indent=2)


def plot_loss_curve(metrics_dict, save_path=None):
    plt.figure(figsize=(10, 5))
    for name, data in metrics_dict.items():
        if 'step' in data:
            plt.plot(data['step'], data['loss'], label=name)
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training Loss Curve')
    plt.grid(True, linestyle='--', alpha=0.5)
    if save_path:
        plt.savefig(save_path)
    plt.close()


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):
        logits = logits[0]
    predictions = np.argmax(logits, axis=-1)
    
    # Only calculate accuracy where labels are not -100
    mask = labels != -100
    
    # Filter predictions and labels
    filtered_preds = predictions[mask]
    filtered_labels = labels[mask]
    
    correct = (filtered_preds == filtered_labels).sum()
    total = mask.sum()
    
    accuracy = correct / total if total > 0 else 0.0
    
    # Calculate Precision, Recall, F1 (weighted average for multi-class/token level)
    precision, recall, f1, _ = precision_recall_fscore_support(
        filtered_labels, filtered_preds, average='weighted', zero_division=0
    )
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }


# ======================================================
# 3. æ•°æ®é¢„å¤„ç† (Correct ChatML + Mask)
# ======================================================

def preprocess_senti(example, tokenizer):
    # 1. ä¿®æ­£æ‹¼å†™é”™è¯¯
    label_text = "positive" if example["label"] == 1 else "negative"

    # æ„å»ºæç¤ºè¯ (prompt) å’Œ å“åº” (response)
    prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
    response = f"{label_text}{tokenizer.eos_token}" 

    full_text = prompt + response

    # ç¼–ç å®Œæ•´æ–‡æœ¬
    tokenized = tokenizer(full_text, truncation=True, max_length=512, padding="max_length")

    # ç¼–ç  prompt ä»¥è·å–é•¿åº¦
    # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†ä¿é™©ï¼Œæœ€å¥½ä¸è¦åŠ  paddingï¼Œç›´æ¥ç®—é•¿åº¦
    prompt_tokenized = tokenizer(prompt, truncation=True, max_length=512)
    prompt_len = len(prompt_tokenized["input_ids"])

    # åˆ›å»º labels
    labels = tokenized["input_ids"][:]

    # === å…³é”®ä¿®æ­£å¼€å§‹ ===
    for i in range(len(labels)):
        # æƒ…å†µ1: å¦‚æœæ˜¯ Padding (attention_mask ä¸º 0)ï¼Œè®¾ç½®ä¸º -100
        if tokenized["attention_mask"][i] == 0:
            labels[i] = -100
        # æƒ…å†µ2: å¦‚æœæ˜¯ Prompt éƒ¨åˆ†ï¼Œè®¾ç½®ä¸º -100
        elif i < prompt_len:
            labels[i] = -100
        # æƒ…å†µ3: å‰©ä¸‹çš„å°±æ˜¯ Response éƒ¨åˆ†ï¼Œä¿ç•™åŸ ID
    # === å…³é”®ä¿®æ­£ç»“æŸ ===

    tokenized["labels"] = labels
    return tokenized


def benchmark_inference(model, tokenizer, dataset, num_samples=50):
    """
    æµ‹è¯•æ¨¡å‹æ¨ç†é€Ÿåº¦
    """
    print(f"\n>>> å¼€å§‹æ¨ç†æ€§èƒ½æµ‹è¯• (Samples={num_samples})...")
    model.eval()
    
    # Select a subset of data
    subset = dataset.select(range(min(num_samples, len(dataset))))
    
    times = []
    
    for i, example in enumerate(subset):
        # Prepare input (only prompt)
        prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        start_time = time.time()
        with torch.no_grad():
            # Generate only a few new tokens
            _ = model.generate(
                **inputs, 
                max_new_tokens=5, 
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        end_time = time.time()
        
        # Skip first warmup sample
        if i > 0:
            times.append(end_time - start_time)
            
    if times:
        avg_time = np.mean(times)
        print(f"Average Inference Time: {avg_time*1000:.2f} ms/sample")
        print(f"Throughput: {1.0/avg_time:.2f} samples/sec")
        return avg_time
    else:
        return 0.0


def evaluate_generative(model, tokenizer, dataset):
    """
    ä½¿ç”¨ç”Ÿæˆå¼è¯„ä¼°è®¡ç®—å‡†ç¡®ç‡ (Generative Evaluation)
    """
    print("\n>>> Starting Generative Evaluation...")
    from tqdm import tqdm

    total = 0
    correct = 0
    
    print(f"Total test samples: {len(dataset)}")
    
    model.eval()

    for i, example in enumerate(tqdm(dataset)):
        label = example["label"] # 0 or 1
        target_text = "positive" if label == 1 else "negative"
        
        prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            # ç”Ÿæˆ
            outputs = model.generate(
                **inputs, 
                max_new_tokens=5, # åªéœ€è¦ç”Ÿæˆå‡ ä¸ª token
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=False # ç¡®å®šæ€§ç”Ÿæˆ
            )
            
        # è§£ç 
        # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
        
        # ç®€å•çš„åŒ…å«åŒ¹é…
        is_correct = False
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ä»¥ç›®æ ‡æ ‡ç­¾å¼€å¤´ (æ›´ä¸¥æ ¼ä¸€ç‚¹)
        if generated_text.startswith(target_text):
            is_correct = True
        # æˆ–è€…åŒ…å«ç›®æ ‡æ ‡ç­¾
        elif target_text in generated_text:
            is_correct = True
            
        if is_correct:
            correct += 1
        total += 1
        
        # æ‰“å°å‰å‡ ä¸ªé”™è¯¯æ¡ˆä¾‹ç”¨äºè°ƒè¯•
        if not is_correct and total <= 5:
             print(f"\n[Fail] Label: {target_text}, Pred: '{generated_text}'")

    accuracy = correct / total
    print("\n" + "="*30)
    print("âœ… Test Results (Generative):")
    print("="*30)
    print(f"Accuracy: {accuracy:.4f} ({correct}/{total})")
    print("="*30)
    return accuracy


# ======================================================
# 4. ä¸»ç¨‹åº
# ======================================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", type=str, default="Qwen/Qwen1.5-1.8B-Chat")
    parser.add_argument("--output_dir", default="./lora_w4a8_out_2stage")
    parser.add_argument("--batch_size", type=int, default=1)  # æ˜¾å­˜ä¼˜åŒ–
    parser.add_argument("--grad_accum", type=int, default=16)  # æ¢¯åº¦ç´¯ç§¯
    parser.add_argument("--lr", type=float, default=2e-4)  # Stage 1 LR
    parser.add_argument("--lr_qat", type=float, default=2e-5)  # Stage 2 LR (Lower)
    parser.add_argument("--group_size", type=int, default=32)  # QA-LoRA Group Size
    parser.add_argument("--lora_rank", type=int, default=8)  # LoRA Rank
    args = parser.parse_args()

    set_seed(42)
    
    # Update output_dir to include configuration
    args.output_dir = f"{args.output_dir}/r{args.lora_rank}_g{args.group_size}"
    os.makedirs(args.output_dir, exist_ok=True)

    # ----------------------------------------------------
    # A. å‡†å¤‡ Tokenizer & Data
    # ----------------------------------------------------
    print(">>> æ­£åœ¨åŠ è½½ Tokenizer å’Œ æ•°æ®é›†...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset("lansinuote/ChnSentiCorp")
    
    # é¢„å¤„ç† (ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ)
    proc_kwargs = {"num_proc": 4, "remove_columns": ["text", "label"]}
    tokenized_train = dataset["train"].map(lambda x: preprocess_senti(x, tokenizer), **proc_kwargs)
    tokenized_test = dataset["test"].map(lambda x: preprocess_senti(x, tokenizer), **proc_kwargs)

    # Debug Check
    print("Debug Label Example:", [l for l in tokenized_train[0]['labels'] if l != -100])

    # ----------------------------------------------------
    # B. Stage 1: Standard FP16 LoRA Warmup
    # ----------------------------------------------------
    print("\n" + "=" * 50)
    print("ğŸš€ Stage 1: Standard FP16 LoRA Training")
    print("=" * 50)

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    # å¿…é¡»å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()

    # é…ç½® LoRA (Target å…¨éƒ¨çº¿æ€§å±‚)
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    peft_config = LoraConfig(
        r=args.lora_rank, lora_alpha=16,
        target_modules=target_modules,
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # Callback
    metrics_cb1 = TrainingMetricsCallback()

    trainer1 = Trainer(
        model=model,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        compute_metrics=compute_metrics,
        args=TrainingArguments(
            output_dir=f"{args.output_dir}/stage1",
            num_train_epochs=1,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.grad_accum,
            learning_rate=args.lr,
            fp16=True,
            save_strategy="epoch",
            evaluation_strategy="no",
            
            logging_steps=10,
            report_to="none",
            # dataloader_num_workers=0
        ),
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        callbacks=[metrics_cb1]
    )

    print(">>> å¼€å§‹ Stage 1 è®­ç»ƒ...")
    trainer1.train()

    with torch.no_grad():
        # ä¿å­˜ Stage 1 LoRA å‚æ•°
        torch.save(model.state_dict(), f"{args.output_dir}/stage1/lora_state_dict.pt")
        evaluate_generative(model, tokenizer, dataset["test"])



    # # ä¿å­˜ Stage 1 æŒ‡æ ‡
    # metrics_cb1.save_metrics(f"{args.output_dir}/stage1_metrics.json")

    # æ¸…ç†æ˜¾å­˜ (å½»åº•åˆ é™¤ model å’Œ trainer)
    del trainer1, model, metrics_cb1
    torch.cuda.empty_cache()
    gc.collect()

    # ----------------------------------------------------
    # C. Stage 2: W4A8 QA-LoRA Training
    # ----------------------------------------------------
    print("\n" + "=" * 50)
    print("ğŸš€ Stage 2: W4A8 QA-LoRA (Group-wise Quant + Adaptation)")
    print("=" * 50)

    # 1. é‡æ–°åŠ è½½å¹²å‡€çš„ Base Model
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()

    # 2. åŸåœ°è½¬æ¢ä¸º W4A8 QA-LoRA ç»“æ„
    # æ³¨æ„ï¼šStage 1 åªæ˜¯ä¸ºäº†è®©æ¨¡å‹ç†Ÿæ‚‰ä»»åŠ¡ã€‚Stage 2 æˆ‘ä»¬ä½¿ç”¨æ–°çš„ QA-LoRA ç»“æ„ä»å¤´(Warmup)å¼€å§‹å¾®è°ƒï¼Œ
    # æˆ–è€…ä½ å¯ä»¥å°è¯•åŠ è½½ Stage 1 çš„å‚æ•°ï¼Œä½†å› ä¸º A çŸ©é˜µå½¢çŠ¶ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–é€šå¸¸æ›´ç®€å•ä¸”æœ‰æ•ˆã€‚
    model = convert_to_qalora_w4a8(
        model,
        r=args.lora_rank,
        lora_alpha=16,
        group_size=args.group_size,
        target_modules=target_modules
    )

    # 3. å†»ç»“ Baseï¼Œåªè®­ç»ƒ QA-LoRA å‚æ•°
    print(">>> é…ç½®å‚æ•°å†»ç»“...")
    for n, p in model.named_parameters():
        if "lora_" in n:
            p.requires_grad = True
        else:
            p.requires_grad = False

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ç¡®è®¤
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    print(f"QA-LoRA Trainable Params: {trainable_params} / {all_params} ({trainable_params / all_params:.2%})")

    metrics_cb2 = TrainingMetricsCallback()

    trainer2 = Trainer(
        model=model,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        compute_metrics=compute_metrics,
        args=TrainingArguments(
            output_dir=f"{args.output_dir}/stage2_w4a8",
            num_train_epochs=1,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.grad_accum,
            learning_rate=args.lr_qat,  # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
            fp16=True,
            logging_steps=10,
            report_to="none",
            save_strategy="epoch",
            evaluation_strategy="no"
        ),
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        callbacks=[metrics_cb2]
    )

    # æ‰‹åŠ¨åˆ›å»ºä¼˜åŒ–å™¨ï¼Œç¡®ä¿åœ¨è®¿é—®trainer2.optimizerä¹‹å‰å·²åˆå§‹åŒ–
    trainer2.create_optimizer()
    
    # æ‰“å°ä¼˜åŒ–å™¨å‚æ•°ç»„ï¼Œç¡®è®¤lora_Aå’Œlora_Bè¢«æ­£ç¡®åŒ…å«
    print(">>> æ£€æŸ¥ä¼˜åŒ–å™¨å‚æ•°ç»„...")
    # å…ˆè·å–æ‰€æœ‰éœ€è¦è®­ç»ƒçš„loraå‚æ•°
    lora_params = {p for n, p in model.named_parameters() if 'lora_' in n and p.requires_grad}
    
    # æ£€æŸ¥è¿™äº›å‚æ•°æ˜¯å¦åœ¨trainerçš„ä¼˜åŒ–å™¨ä¸­
    optimizer = trainer2.optimizer
    for i, param_group in enumerate(optimizer.param_groups):
        print(f"  Param Group {i}:")
        print(f"    - LR: {param_group['lr']}")
        # æ£€æŸ¥è¯¥ç»„åŒ…å«å¤šå°‘loraå‚æ•°
        lora_count = sum(1 for param in param_group['params'] if param in lora_params)
        print(f"    - LoRA params in group: {lora_count}")
    
    # æ‰“å°ä¸€äº›å…·ä½“çš„loraå‚æ•°ä¿¡æ¯
    print(">>> å¯è®­ç»ƒçš„LoRAå‚æ•°åˆ—è¡¨ï¼š")
    for n, p in model.named_parameters():
        if 'lora_' in n and p.requires_grad:
            print(f"  - {n}: {p.shape}, requires_grad={p.requires_grad}")
    
    print(">>> å¼€å§‹ Stage 2 (W4A8 QAT) è®­ç»ƒ...")
    trainer2.train()

    # ä¿å­˜ Stage 2 æŒ‡æ ‡å’Œæ¨¡å‹çŠ¶æ€
    metrics_cb2.save_metrics(f"{args.output_dir}/stage2_metrics.json")
    torch.save(model.state_dict(), f"{args.output_dir}/stage2_w4a8/qalora_state_dict.pt")
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    save_merged_model(model, f"{args.output_dir}/stage2_w4a8")

    # ----------------------------------------------------
    # D. å¯è§†åŒ–
    # ----------------------------------------------------
    print(">>> ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    all_metrics = {
        "Stage1_FP16_LoRA": metrics_cb1.metrics,
        "Stage2_W4A8_QALoRA": metrics_cb2.metrics
    }
    plot_loss_curve(all_metrics, save_path=f"{args.output_dir}/loss_curve_w4a8.png")

    # ----------------------------------------------------
    # E. æ¨ç†æµ‹è¯•
    # ----------------------------------------------------
    benchmark_inference(model, tokenizer, dataset["test"], num_samples=50)

    # ----------------------------------------------------
    # F. ç”Ÿæˆå¼è¯„ä¼°
    # ----------------------------------------------------
    evaluate_generative(model, tokenizer, dataset["test"])

    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼è¾“å‡ºç›®å½•: {args.output_dir}")


if __name__ == "__main__":
    main()