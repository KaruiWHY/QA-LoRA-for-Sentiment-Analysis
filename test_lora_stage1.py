# -*- coding: utf-8 -*-
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import argparse
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

# ======================================================
# 0. æ˜¾å­˜ç›‘æ§å·¥å…· (æ–°å¢)
# ======================================================
def print_gpu_memory(tag=""):
    """æ‰“å°å½“å‰å’Œå³°å€¼æ˜¾å­˜å ç”¨"""
    if torch.cuda.is_available():
        # ç¡®ä¿åŒæ­¥ï¼Œè·å–å‡†ç¡®å€¼
        torch.cuda.synchronize()
        max_mem = torch.cuda.max_memory_allocated() / 1024**3
        current_mem = torch.cuda.memory_allocated() / 1024**3
        print(f"\nğŸ“Š [{tag}] Max GPU Memory: {max_mem:.2f} GB | Current: {current_mem:.2f} GB")
        # é‡ç½®å³°å€¼ç»Ÿè®¡ï¼Œä»¥ä¾¿æµ‹é‡ä¸‹ä¸€é˜¶æ®µçš„ç‹¬ç«‹å³°å€¼
        torch.cuda.reset_peak_memory_stats()

# ======================================================
# 0. ç¯å¢ƒé…ç½®
# ======================================================
# os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# å¼ºåˆ¶è®¾ç½®ä¸´æ—¶ç›®å½•ï¼Œé˜²æ­¢ No usable temporary directory found
# _custom_temp_dir = os.path.join(os.getcwd(), "tmp_cache")
# os.makedirs(_custom_temp_dir, exist_ok=True)
# os.environ["TMPDIR"] = _custom_temp_dir
# os.environ["TEMP"] = _custom_temp_dir
# os.environ["TMP"] = _custom_temp_dir
# print(f">>> Temporary directory set to: {_custom_temp_dir}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen1.5-1.8B-Chat")
    # é»˜è®¤æŒ‡å‘ Stage 1 çš„ checkpoint
    parser.add_argument("--lora_path", type=str, default="./lora_w4a8_out_2stage/r8_g32/stage1/checkpoint-600")
    args = parser.parse_args()

    print_gpu_memory("Start")

    print(f"ğŸš€ Testing Stage 1 LoRA Model")
    print(f"Base Model: {args.base_model}")
    print(f"LoRA Path:  {args.lora_path}")

    # ----------------------------------------------------
    # 1. åŠ è½½ Tokenizer & Dataset
    # ----------------------------------------------------
    print(">>> Loading Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(">>> Loading Dataset: lansinuote/ChnSentiCorp")
    dataset = load_dataset("lansinuote/ChnSentiCorp")
    test_subset = dataset["test"]
    print(f"Total test samples: {len(test_subset)}")

    # ----------------------------------------------------
    # 2. åŠ è½½æ¨¡å‹ (Base + LoRA)
    # ----------------------------------------------------
    print(">>> Loading Base Model...")
    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    print_gpu_memory("Base Model Loaded")

    print(f">>> Loading LoRA Adapter from {args.lora_path}...")
    try:
        model = PeftModel.from_pretrained(model, args.lora_path)
        print("âœ… LoRA Adapter loaded successfully.")
    except Exception as e:
        print(f"âŒ Failed to load LoRA adapter: {e}")
        return

    print_gpu_memory("LoRA Adapter Loaded")

    model.eval()
    
    # ----------------------------------------------------
    # 3. ç”Ÿæˆå¼è¯„ä¼° (Generative Evaluation)
    # ----------------------------------------------------
    print("\n>>> Starting Generative Evaluation...")
    
    total = 0
    correct = 0
    
    for i, example in enumerate(tqdm(test_subset)):
        label = example["label"] # 0 or 1
        target_text = "positive" if label == 1 else "negative"
        
        # æ„å»º Prompt (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
        prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            # ç”Ÿæˆ
            outputs = model.generate(
                **inputs, 
                max_new_tokens=5, # åªéœ€è¦ç”Ÿæˆå‡ ä¸ª token
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=False # ç¡®å®šæ€§ç”Ÿæˆ
            )
            
        # è§£ç  (åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†)
        generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
        
        # ç®€å•çš„åŒ…å«åŒ¹é…
        is_correct = False
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ä»¥ç›®æ ‡æ ‡ç­¾å¼€å¤´
        if generated_text.startswith(target_text):
            is_correct = True
        # æˆ–è€…åŒ…å«ç›®æ ‡æ ‡ç­¾
        elif target_text in generated_text:
            is_correct = True
            
        if is_correct:
            correct += 1
        total += 1
        
        # æ‰“å°å‰å‡ ä¸ªé”™è¯¯æ¡ˆä¾‹ç”¨äºè°ƒè¯•
        if not is_correct and total <= 5:
             print(f"\n[Fail] Label: {target_text}, Pred: '{generated_text}'")

    accuracy = correct / total
    
    print_gpu_memory("Inference Finished")

    print("\n" + "="*30)
    print("âœ… Test Results (Stage 1 LoRA):")
    print("="*30)
    print(f"Accuracy: {accuracy:.4f} ({correct}/{total})")
    print("="*30)

if __name__ == "__main__":
    main()
