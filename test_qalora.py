# -*- coding: utf-8 -*-
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import torch.nn as nn
import torch.nn.functional as F
import argparse
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from sklearn.metrics import precision_recall_fscore_support
from peft.tuners.lora.layer import Linear as LoRALinear

# ======================================================
# 0. æ˜¾å­˜ç›‘æ§å·¥å…· (æ–°å¢)
# ======================================================
def print_gpu_memory(tag=""):
    """æ‰“å°å½“å‰å’Œå³°å€¼æ˜¾å­˜å ç”¨"""
    if torch.cuda.is_available():
        # ç¡®ä¿åŒæ­¥ï¼Œè·å–å‡†ç¡®å€¼
        torch.cuda.synchronize()
        max_mem = torch.cuda.max_memory_allocated() / 1024**3
        current_mem = torch.cuda.memory_allocated() / 1024**3
        print(f"\nğŸ“Š [{tag}] Max GPU Memory: {max_mem:.2f} GB | Current: {current_mem:.2f} GB")
        # é‡ç½®å³°å€¼ç»Ÿè®¡ï¼Œä»¥ä¾¿æµ‹é‡ä¸‹ä¸€é˜¶æ®µçš„ç‹¬ç«‹å³°å€¼
        torch.cuda.reset_peak_memory_stats()

# ======================================================
# 1. æ ¸å¿ƒé‡åŒ–ç»„ä»¶ (å¤åˆ¶è‡ª QAT-LoRA.py)
# ======================================================

class FakeQuantSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale):
        scale = torch.clamp(scale, min=1e-8)
        q = torch.clamp(torch.round(x / scale), -127, 127)
        return q * scale

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output, None

def calc_scale_tokenwise(x, eps=1e-8):
    max_abs = x.detach().abs().amax(dim=-1, keepdim=True)
    return torch.clamp(max_abs / 127.0, min=eps)

class QALoRALayer(nn.Module):
    def __init__(self, base_layer: nn.Module, r: int = 8, lora_alpha: int = 16, group_size: int = 32):
        super().__init__()
        self.in_features = base_layer.in_features
        self.out_features = base_layer.out_features
        self.group_size = group_size

        if self.in_features % group_size != 0:
            raise ValueError(f"in_features ({self.in_features}) must be divisible by group_size ({group_size})")

        self.weight = base_layer.weight
        # æ³¨æ„ï¼šæµ‹è¯•æ—¶ä¸éœ€è¦æ¢¯åº¦
        self.weight.requires_grad = False

        self.r = r
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / r
        self.reduced_dim = self.in_features // group_size

        self.lora_A = nn.Parameter(torch.zeros(r, self.reduced_dim))
        self.lora_B = nn.Parameter(torch.zeros(self.out_features, r))
        
        self.register_buffer('quant_scale', None)
        self.register_buffer('quant_zero', None)

    def fake_quant_activation(self, x):
        scale = calc_scale_tokenwise(x)
        return FakeQuantSTE.apply(x, scale)

    def fake_quant_weight_asym(self, w):
        out_dim, in_dim = w.shape
        w_reshaped = w.reshape(out_dim, in_dim // self.group_size, self.group_size)
        max_val = w_reshaped.amax(dim=-1, keepdim=True)
        min_val = w_reshaped.amin(dim=-1, keepdim=True)
        alpha = (max_val - min_val) / 15.0
        alpha = torch.clamp(alpha, min=1e-5)
        beta = min_val
        w_int = ((w_reshaped - beta) / alpha).round().clamp(0, 15)
        w_recon = w_int * alpha + beta
        w_recon = w_recon.reshape(out_dim, in_dim)
        return w_recon, alpha, beta

    def forward(self, x):
        x_q = self.fake_quant_activation(x)
        w_q, _, _ = self.fake_quant_weight_asym(self.weight)
        base_out = F.linear(x_q, w_q)
        b, s, d = x_q.shape
        x_reshaped = x_q.reshape(b, s, d // self.group_size, self.group_size)
        x_pooled = x_reshaped.sum(dim=-1)
        lora_input = x_pooled.to(self.lora_A.dtype)
        lora_out = (lora_input @ self.lora_A.T @ self.lora_B.T) * self.scaling
        return base_out + lora_out.to(base_out.dtype)

def convert_to_qalora_w4a8(model, r=8, lora_alpha=16, group_size=32, target_modules=None):
    if target_modules is None:
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    print(f"ğŸ”„ æ­£åœ¨å°†æ¨¡å‹è½¬æ¢ä¸º W4A8 QA-LoRA (Group Size={group_size})...")

    def replace_module(module, current_name=""):
        for name, child in module.named_children():
            full_name = f"{current_name}.{name}" if current_name else name
            if isinstance(child, (LoRALinear, nn.Linear)):
                is_target = any(t in name for t in target_modules)
                if isinstance(child, LoRALinear) and is_target:
                    base_layer = child.base_layer
                    new_layer = QALoRALayer(base_layer, r=r, lora_alpha=lora_alpha, group_size=group_size)
                    new_layer = new_layer.to(base_layer.weight.device).to(base_layer.weight.dtype)
                    setattr(module, name, new_layer)
                elif isinstance(child, nn.Linear) and is_target:
                    new_layer = QALoRALayer(child, r=r, lora_alpha=lora_alpha, group_size=group_size)
                    new_layer = new_layer.to(child.weight.device).to(child.weight.dtype)
                    setattr(module, name, new_layer)
            else:
                replace_module(child, full_name)

    replace_module(model)
    return model

# ======================================================
# 2. æ•°æ®å¤„ç†ä¸è¯„ä¼° (å¤åˆ¶è‡ª QAT-LoRA.py)
# ======================================================

def preprocess_senti(example, tokenizer):
    label_text = "positive" if example["label"] == 1 else "negative"
    prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
    response = f"{label_text}{tokenizer.eos_token}" 
    full_text = prompt + response
    tokenized = tokenizer(full_text, truncation=True, max_length=512, padding="max_length")
    prompt_tokenized = tokenizer(prompt, truncation=True, max_length=512)
    prompt_len = len(prompt_tokenized["input_ids"])
    labels = tokenized["input_ids"][:]
    for i in range(len(labels)):
        if tokenized["attention_mask"][i] == 0:
            labels[i] = -100
        elif i < prompt_len:
            labels[i] = -100
    tokenized["labels"] = labels
    return tokenized

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):
        logits = logits[0]
    predictions = np.argmax(logits, axis=-1)
    mask = labels != -100
    filtered_preds = predictions[mask]
    filtered_labels = labels[mask]
    correct = (filtered_preds == filtered_labels).sum()
    total = mask.sum()
    accuracy = correct / total if total > 0 else 0.0
    precision, recall, f1, _ = precision_recall_fscore_support(
        filtered_labels, filtered_preds, average='weighted', zero_division=0
    )
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

# ======================================================
# 3. ä¸»ç¨‹åº
# ======================================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen1.5-1.8B-Chat")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="Path to qalora_state_dict.pt")
    parser.add_argument("--lora_rank", type=int, default=8)
    parser.add_argument("--group_size", type=int, default=32)
    parser.add_argument("--batch_size", type=int, default=4)
    args = parser.parse_args()

    print_gpu_memory("Start")

    print(f">>> Loading Tokenizer: {args.base_model}")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(">>> Loading Dataset: lansinuote/ChnSentiCorp")
    dataset = load_dataset("lansinuote/ChnSentiCorp")
    
    # é¢„å¤„ç†æµ‹è¯•é›†
    print(">>> Preprocessing test set...")
    test_dataset = dataset["test"].map(
        lambda x: preprocess_senti(x, tokenizer), 
        num_proc=4,
        remove_columns=["text", "label"]
    )

    print(f">>> Loading Base Model: {args.base_model}")
    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    print_gpu_memory("Base Model Loaded")

    # è½¬æ¢ç»“æ„
    print(f">>> Converting to QA-LoRA (r={args.lora_rank}, g={args.group_size})...")
    model = convert_to_qalora_w4a8(model, r=args.lora_rank, group_size=args.group_size)
    print_gpu_memory("Converted to QA-LoRA")

    # åŠ è½½æƒé‡
    print(f">>> Loading weights from {args.checkpoint_path}")
    
    if args.checkpoint_path.endswith(".safetensors"):
        from safetensors.torch import load_file
        state_dict = load_file(args.checkpoint_path)
    else:
        state_dict = torch.load(args.checkpoint_path, map_location="cpu")
    
    # åŠ è½½æƒé‡ (strict=False å› ä¸º state_dict å¯èƒ½åŒ…å«ä¸€äº›å¤šä½™çš„é”®ï¼Œæˆ–è€…ç¼ºå°‘ base model çš„å†»ç»“é”®)
    # å…³é”®æ˜¯ç¡®ä¿ lora_A, lora_B ç­‰è¢«æ­£ç¡®åŠ è½½
    print(">>> Loading state dict...")
    load_result = model.load_state_dict(state_dict, strict=False)
    
    print(f"Missing keys: {len(load_result.missing_keys)}")
    if len(load_result.missing_keys) > 0:
        print(f"Sample missing keys: {load_result.missing_keys[:5]}")
        
    print(f"Unexpected keys: {len(load_result.unexpected_keys)}")
    if len(load_result.unexpected_keys) > 0:
        print(f"Sample unexpected keys: {load_result.unexpected_keys[:5]}")

    print_gpu_memory("Weights Loaded")

    # æ£€æŸ¥ LoRA æƒé‡æ˜¯å¦æˆåŠŸåŠ è½½ (æ£€æŸ¥ç¬¬ä¸€ä¸ª QALoRALayer çš„ lora_B æ˜¯å¦å…¨ä¸º 0)
    for name, module in model.named_modules():
        if isinstance(module, QALoRALayer):
            if module.lora_B.sum() == 0:
                print(f"âš ï¸ WARNING: {name}.lora_B is all zeros! LoRA weights might not be loaded correctly.")
            else:
                print(f"âœ… {name}.lora_B loaded successfully (sum={module.lora_B.sum().item():.4f}).")
            break # åªæ£€æŸ¥ç¬¬ä¸€ä¸ª

    # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
    model.eval()
    torch.cuda.empty_cache()

    # è¯„ä¼° (ä½¿ç”¨ç”Ÿæˆå¼è¯„ä¼° Generative Evaluation)
    print(">>> Starting Generative Evaluation...")
    from tqdm import tqdm

    total = 0
    correct = 0
    
    # ä½¿ç”¨åŸå§‹æ•°æ®é›†çš„ test splitï¼Œè€Œä¸æ˜¯é¢„å¤„ç†åçš„ tokenized dataset
    # å› ä¸ºæˆ‘ä»¬éœ€è¦åŸå§‹æ–‡æœ¬æ¥æ„å»º prompt
    test_subset = dataset["test"]
    
    print(f"Total test samples: {len(test_subset)}")

    for i, example in enumerate(tqdm(test_subset)):
        label = example["label"] # 0 or 1
        target_text = "positive" if label == 1 else "negative"
        
        prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            # ç”Ÿæˆ
            outputs = model.generate(
                **inputs, 
                max_new_tokens=5, # åªéœ€è¦ç”Ÿæˆå‡ ä¸ª token
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=False # ç¡®å®šæ€§ç”Ÿæˆ
            )
            
        # è§£ç 
        # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
        
        # ç®€å•çš„åŒ…å«åŒ¹é…
        is_correct = False
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ä»¥ç›®æ ‡æ ‡ç­¾å¼€å¤´ (æ›´ä¸¥æ ¼ä¸€ç‚¹)
        if generated_text.startswith(target_text):
            is_correct = True
        # æˆ–è€…åŒ…å«ç›®æ ‡æ ‡ç­¾
        elif target_text in generated_text:
            is_correct = True
            
        if is_correct:
            correct += 1
        total += 1
        
        # æ‰“å°å‰å‡ ä¸ªé”™è¯¯æ¡ˆä¾‹ç”¨äºè°ƒè¯•
        if not is_correct and total <= 10:
             print(f"\n[Fail] Label: {target_text}, Pred: '{generated_text}'")

    accuracy = correct / total
    
    print_gpu_memory("Inference Finished")

    print("\n" + "="*30)
    print("âœ… Test Results (Generative):")
    print("="*30)
    print(f"Accuracy: {accuracy:.4f} ({correct}/{total})")
    print("="*30)

if __name__ == "__main__":
    main()
