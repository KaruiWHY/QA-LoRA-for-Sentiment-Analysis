# -*- coding: utf-8 -*-                          # æŒ‡å®šæ–‡ä»¶ç¼–ç ä¸º UTF-8ï¼Œç¡®ä¿ä¸­æ–‡æ³¨é‡Šä¸å­—ç¬¦ä¸²ä¸ä¼šä¹±ç 
"""
ex4_joint_qat_qlora_train_fp16.py
---------------------------------
è”åˆè®­ç»ƒï¼šQATï¼ˆæ¿€æ´»ä¼ªé‡åŒ–ï¼‰ + LoRAï¼ˆFP16 å¯è®­ç»ƒé€‚é…å™¨ï¼‰
é€‚é… AMD ROCm / CUDAï¼Œä¸ä¾èµ– bitsandbytesã€‚

âœ… åŠŸèƒ½è¦ç‚¹
- æ¨¡å‹ä¸»å¹²ï¼šFP16ï¼Œå…¨æƒé‡å†»ç»“ï¼›
- é€‚é…å™¨ï¼šLoRA æ³¨å…¥ï¼Œä»…è®­ç»ƒ LoRA å‚æ•°ï¼›
- QATï¼šå¯¹å­å±‚è¾“å…¥åšæ¿€æ´»ä¼ªé‡åŒ– (FakeQuant + STE)ï¼Œæ¨¡æ‹Ÿä½æ¯”ç‰¹æ¨ç†ã€‚

ä¾èµ–ï¼š
    pip install torch transformers peft accelerate datasets
"""

import os                                          # æ–‡ä»¶ä¸è·¯å¾„æ“ä½œ
import torch                                       # PyTorch ä¸»åº“
import torch.nn as nn                              # ç¥ç»ç½‘ç»œç»„ä»¶
import torch.nn.functional as F                    # å¸¸ç”¨å‡½æ•°åº“ (å¦‚æ¿€æ´»ã€æŸå¤±)
import random, argparse, json                      # éšæœºæ•°ã€å‘½ä»¤è¡Œå‚æ•°è§£æã€JSON ä¿å­˜
from typing import List, Tuple                     # ç±»å‹æ³¨è§£å·¥å…·
from datasets import Dataset,load_dataset                       # Hugging Face æ•°æ®é›†å·¥å…·
from transformers import (                         # å¯¼å…¥ Transformers æ¨¡å—
    AutoTokenizer,                                 # è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨
    AutoModelForCausalLM,                          # è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹
    Trainer,                                       # è®­ç»ƒå™¨å°è£…ç±»
    TrainingArguments,                             # è®­ç»ƒé…ç½®å‚æ•°
    DataCollatorForLanguageModeling,               # æ•°æ®æ•´ç†å™¨ï¼ˆè‡ªåŠ¨å¡«å……ã€å¯¹é½ï¼‰
)
from peft import LoraConfig, get_peft_model        # å¯¼å…¥ PEFT åº“çš„ LoRA æ¨¡å—
from peft.tuners.lora.layer import Linear as LoRALinear  # è®¿é—® LoRA çš„çº¿æ€§å±‚å®ç°

# ======================================================
# 1  å®ç”¨å‡½æ•°
# ======================================================
def set_seed(seed=42):                             # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°æ€§
    random.seed(seed)                              # Python éšæœºæ•°ç§å­
    torch.manual_seed(seed)                        # PyTorch CPU éšæœºç§å­
    torch.cuda.manual_seed_all(seed)               # GPU éšæœºç§å­ï¼ˆå¤šå¡æƒ…å†µï¼‰

def print_trainable_parameters(model):             # æ‰“å°æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹
    trainable, total = 0, 0
    for p in model.parameters():                   # éå†æ‰€æœ‰å‚æ•°
        total += p.numel()                         # ç´¯åŠ å‚æ•°æ€»æ•°
        if p.requires_grad:                        # åˆ¤æ–­æ˜¯å¦å¯è®­ç»ƒ
            trainable += p.numel()
    print(f"ğŸ§® å¯è®­ç»ƒå‚æ•°: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)")

# ======================================================
# 2 FakeQuant + STE (ä»…æ¿€æ´»)
# ======================================================
class FakeQuantSTE(torch.autograd.Function):       # å®šä¹‰è‡ªå®šä¹‰ä¼ªé‡åŒ–å‡½æ•°ï¼ˆæ”¯æŒåä¼ ï¼‰
    @staticmethod
    def forward(ctx, x, scale):                    # å‰å‘ä¼ æ’­ï¼šæ¨¡æ‹Ÿé‡åŒ– + åé‡åŒ–
        scale = torch.clamp(scale, min=1e-8)       # é˜²æ­¢ scale è¿‡å°
        q = torch.clamp(torch.round(x / scale), -127, 127)  # é‡åŒ–åˆ° INT8 åŒºé—´
        return q * scale                           # åé‡åŒ–å› FP16 ç©ºé—´
    @staticmethod
    def backward(ctx, grad_output):                # åå‘ä¼ æ’­ï¼šSTEï¼ˆç›´é€šä¼°è®¡ï¼‰
        return grad_output, None                   # å¿½ç•¥ scale æ¢¯åº¦ï¼Œåªä¿ç•™ x çš„æ¢¯åº¦

def calc_scale_tensorwise(x, eps=1e-8):            # è®¡ç®—å¼ é‡çº§ scale å€¼
    max_abs = x.detach().abs().amax()              # è·å–ç»å¯¹å€¼æœ€å¤§å€¼
    return torch.clamp(max_abs / 127.0, min=eps)   # æ˜ å°„åˆ° INT8 åŠ¨æ€èŒƒå›´

class QActWrapper(nn.Module):                      # æ¿€æ´»ä¼ªé‡åŒ–åŒ…è£…å™¨
    """å¯¹è¾“å…¥æ¿€æ´»è¿›è¡ŒINT8ä¼ªé‡åŒ–ï¼ˆTensorçº§ï¼‰ï¼Œä¿æŒæƒé‡ä¸åŠ¨"""
    def __init__(self, submodule: nn.Module):
        super().__init__()
        self.sub = submodule                       # ä¿å­˜åŸå­å±‚
    def forward(self, x, *args, **kwargs):          # å‰å‘æ—¶å…ˆé‡åŒ–è¾“å…¥
        scale = calc_scale_tensorwise(x)            # è®¡ç®—é‡åŒ–æ¯”ä¾‹
        x_q = FakeQuantSTE.apply(x, scale)          # æ‰§è¡Œé‡åŒ–ä»¿çœŸ
        return self.sub(x_q, *args, **kwargs)       # è°ƒç”¨åŸå­å±‚ç»§ç»­å‰ä¼ 

# ======================================================
# 3 QAT é€‰æ‹©é€»è¾‘
# ======================================================
ATTN_KEYS = ["q_proj", "k_proj", "v_proj", "o_proj"]  # æ³¨æ„åŠ›å±‚å…³é”®è¯
FFN_KEYS  = ["up_proj", "gate_proj", "down_proj", "w1", "w2", "w3"]  # å‰é¦ˆå±‚å…³é”®è¯

def should_wrap(name: str, qat_targets: List[str]) -> bool:  # åˆ¤æ–­æ˜¯å¦åº”é‡åŒ–è¯¥å±‚
    lname = name.lower()
    want_attn = "attn" in qat_targets
    want_ffn  = "ffn" in qat_targets
    if want_attn and any(k in lname for k in ATTN_KEYS): return True
    if want_ffn and any(k in lname for k in FFN_KEYS): return True
    return False

def get_parent_by_name(model, name) -> Tuple[nn.Module, str]:  # è·å–æ¨¡å—çˆ¶çº§å¯¹è±¡
    parts = name.split(".")
    parent = model
    for p in parts[:-1]:
        if not hasattr(parent, p): return None, None
        parent = getattr(parent, p)
    return parent, parts[-1]

def wrap_for_qat_after_lora(model, qat_targets):   # åŒ…è£¹æŒ‡å®šå±‚çš„æ¿€æ´»ä¼ªé‡åŒ–é€»è¾‘
    """ä»…åŒ…è£¹ LoRA base_layer + æ™®é€š Linear"""
    replaced = []                                  # è®°å½•è¢«æ›¿æ¢çš„å±‚
    for name, module in list(model.named_modules()):  # éå†æ‰€æœ‰å­å±‚
        if isinstance(module, LoRALinear) and should_wrap(name, qat_targets):  # é’ˆå¯¹ LoRA å­å±‚
            if hasattr(module, "base_layer"):
                module.base_layer = QActWrapper(module.base_layer)  # åŒ…è£¹æ¿€æ´»é‡åŒ–
                replaced.append(f"{name}.base_layer")
        elif "lora" in name.lower():               # è·³è¿‡ LoRA è‡ªèº«å®šä¹‰å±‚
            continue
        elif should_wrap(name, qat_targets):       # å¯¹æ™®é€š Linear å±‚
            parent, key = get_parent_by_name(model, name)
            if parent and isinstance(getattr(parent, key), nn.Module):
                setattr(parent, key, QActWrapper(getattr(parent, key)))  # æ›¿æ¢ä¸ºé‡åŒ–åŒ…è£…å±‚
                replaced.append(name)
    print(f"ğŸ”§ å·²åŒ…è£¹ QAT æ¿€æ´»ä¼ªé‡åŒ–å­å±‚: {len(replaced)} å±‚")
    for n in replaced[:20]:                        # æ‰“å°å‰ 20 å±‚åŒ…è£¹ä¿¡æ¯
        print("  â€¢", n)
    if len(replaced) > 20: print("  â€¢ ... (çœç•¥)")

# ======================================================
# 4 æ„å»ºæ•°æ®é›†
# ======================================================        

def preprocess_senti(example, tokenizer):
    # å°† label (0 æˆ– 1) è½¬æ¢ä¸ºæ–‡æœ¬
    label_text = "positive" if example["label"] == 1 else "negative"

    # æ„å»ºæç¤ºè¯ (prompt) å’Œ å“åº” (response)
    prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{example['text']}\næƒ…æ„Ÿï¼š"
    response = f"{label_text}{tokenizer.eos_token}"  # åŠ ä¸Šç»“æŸç¬¦

    # Causal LM è®­ç»ƒï¼šå°† prompt å’Œ response æ‹¼æ¥åœ¨ä¸€èµ·
    full_text = prompt + response

    # 1. ç¼–ç å®Œæ•´æ–‡æœ¬
    tokenized = tokenizer(full_text, truncation=True, max_length=512, padding="max_length")

    # 2. ç¼–ç æç¤ºè¯ (ç”¨äºè®¡ç®—æ ‡ç­¾)
    prompt_tokenized = tokenizer(prompt, truncation=True, max_length=512, padding="max_length")
    prompt_len = sum(prompt_tokenized.attention_mask)

    # 3. åˆ›å»ºæ ‡ç­¾
    # ç›®æ ‡ï¼šè®©æ¨¡å‹åªå­¦ä¹ é¢„æµ‹ response éƒ¨åˆ†
    # æ–¹æ³•ï¼šå°† prompt éƒ¨åˆ†çš„ token å¯¹åº”çš„ labels è®¾ä¸º -100 (å¿½ç•¥)
    labels = tokenized["input_ids"][:]
    labels[:prompt_len] = [-100] * prompt_len

    tokenized["labels"] = labels
    return tokenized


# ======================================================
# 5 ä¸»æµç¨‹
# ======================================================
def main():
    parser = argparse.ArgumentParser()             # åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨
    parser.add_argument("--model_name_or_path", type=str,
                        default=r"Qwen/Qwen1.5-1.8B-Chat",
                        help="åŸå§‹HFæ¨¡å‹è·¯å¾„") # æ¨¡å‹è·¯å¾„æˆ–åç§°
    parser.add_argument("--output_dir", default="./joint-fp16-out")  # è¾“å‡ºç›®å½•
    parser.add_argument("--epochs", type=int, default=1)        # è®­ç»ƒè½®æ•°
    parser.add_argument("--batch_size", type=int, default=1)    # æ‰¹å¤§å°
    parser.add_argument("--grad_accum", type=int, default=4)    # æ¢¯åº¦ç´¯è®¡æ­¥æ•°
    parser.add_argument("--lr", type=float, default=2e-4)       # å­¦ä¹ ç‡
    parser.add_argument("--max_length", type=int, default=512)  # æœ€å¤§åºåˆ—é•¿åº¦
    parser.add_argument("--seed", type=int, default=42)         # éšæœºç§å­
    parser.add_argument("--qat_targets", type=str, default="attn,ffn")  # æŒ‡å®šé‡åŒ–ç›®æ ‡
    args = parser.parse_args()                    # è§£æå‚æ•°

    set_seed(args.seed)                           # å›ºå®šéšæœºç§å­
    os.makedirs(args.output_dir, exist_ok=True)   # åˆ›å»ºè¾“å‡ºç›®å½•

    print(f"ğŸ”¹ åŠ è½½ FP16 æ¨¡å‹ï¼š{args.model_name_or_path}")
    model = AutoModelForCausalLM.from_pretrained( # åŠ è½½åŸºç¡€è¯­è¨€æ¨¡å‹
        args.model_name_or_path,
        torch_dtype=torch.float16,                # ä½¿ç”¨åŠç²¾åº¦æƒé‡
        device_map="auto",                        # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆæ”¯æŒå¤š GPUï¼‰
        trust_remote_code=True,                   # å…è®¸è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)  # åŠ è½½åˆ†è¯å™¨
    if tokenizer.pad_token is None:               # è‹¥æ—  pad_token åˆ™è¡¥é½
        tokenizer.pad_token = tokenizer.eos_token

    # æ³¨å…¥ LoRA æ¨¡å—
    print(">>>æ³¨å…¥ LoRA é€‚é…å™¨ ...")
    lora_cfg = LoraConfig(                        # å®šä¹‰ LoRA å‚æ•°é…ç½®
        r=8, lora_alpha=16, lora_dropout=0.05,    # ä½ç§©åˆ†è§£ã€ç¼©æ”¾ä¸dropout
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],      # åœ¨æ‰€æœ‰çº¿æ€§å±‚æ³¨å…¥ LoRA
        bias="none", task_type="CAUSAL_LM"        # è¯­è¨€å»ºæ¨¡ä»»åŠ¡
    )
    model = get_peft_model(model, lora_cfg)       # å°† LoRA æ¨¡å—æ³¨å…¥æ¨¡å‹
    print_trainable_parameters(model)             # æ‰“å°å¯è®­ç»ƒå‚æ•°å æ¯”

    # åŒ…è£¹æ¿€æ´»ä¼ªé‡åŒ–æ¨¡å—
    qat_targets = [s.strip().lower() for s in args.qat_targets.split(",")]
    wrap_for_qat_after_lora(model, qat_targets)

    # æ•°æ®å‡†å¤‡
    dataset_name = "lansinuote/ChnSentiCorp"                            # æ•°æ®é›†åç§°
    print(f">>>æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name}...")                  
    dataset = load_dataset(dataset_name)                                # æ„å»ºæ•°æ®é›†
    dataset = dataset["train"].train_test_split(test_size=0.1, seed=42) # åˆ‡åˆ†æµ‹è¯•æ•°æ®é›†

    tokenized_dataset = dataset.map(lambda x: preprocess_senti(x, tokenizer))

    collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
    )
    # è®­ç»ƒé…ç½®
    train_args = TrainingArguments(
        output_dir=args.output_dir,               # è¾“å‡ºç›®å½•
        num_train_epochs=args.epochs,             # è®­ç»ƒè½®æ•°
        per_device_train_batch_size=args.batch_size,  # æ¯å¡æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps=args.grad_accum,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate=args.lr,                    # å­¦ä¹ ç‡
        fp16=True,                                # å¯ç”¨åŠç²¾åº¦è®­ç»ƒ
        logging_steps=5,                          # æ—¥å¿—é—´éš”
        save_steps=100,                           # æ¨¡å‹ä¿å­˜æ­¥æ•°
        evaluation_strategy="no",                 # ä¸å¯ç”¨è¯„ä¼°
        save_total_limit=1,                       # æœ€å¤šä¿å­˜1ä»½æƒé‡
        report_to="none",                         # ä¸ä¸Šä¼ è‡³wandb
        dataloader_num_workers=0,                 # å•çº¿ç¨‹
    )

    trainer = Trainer(                            # æ„å»º Hugging Face Trainer
        model=model,
        args=train_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        tokenizer=tokenizer,
        data_collator=collator,
    )

    print(">>>å¯åŠ¨ FP16 + LoRA + QAT è”åˆè®­ç»ƒ ...")
    trainer.train()                               # å¼€å§‹è®­ç»ƒ
    print(">>>è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹ä¸­ ...")
    model.save_pretrained(args.output_dir)        # ä¿å­˜æ¨¡å‹æƒé‡
    tokenizer.save_pretrained(args.output_dir)    # ä¿å­˜åˆ†è¯å™¨

    # ç®€å•æ¨ç†æµ‹è¯•
    prompts = [
        "åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®º:æˆ‘çœŸçš„æ˜¯å—å¤Ÿäº†ï¼\næƒ…æ„Ÿï¼š",
        "åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®º:è¿™é‡Œçš„æ°›å›´çœŸçš„ä¸é”™ã€‚\næƒ…æ„Ÿï¼š",
        "åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®º:æˆ‘è§‰å¾—è™½ç„¶æ²¡ä»€ä¹ˆæ„æ€ï¼Œä½†æ•´ä½“è¿˜å¯ä»¥å§\næƒ…æ„Ÿï¼š"
    ]
    model.eval()                                  # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
    for p in prompts:
        inputs = tokenizer(p, return_tensors="pt").to(model.device)  # ç¼–ç è¾“å…¥
        with torch.no_grad():                     # å…³é—­æ¢¯åº¦è®¡ç®—
            out = model.generate(                 # ç”Ÿæˆæ–‡æœ¬
                **inputs, max_new_tokens=64, do_sample=True,
                temperature=0.7, top_p=0.9,
                repetition_penalty = 1.2  # æƒ©ç½šé‡å¤è¯ (1.0è¡¨ç¤ºæ— æƒ©ç½šï¼Œ1.2è¡¨ç¤ºè½»å¾®æƒ©ç½š)
            )
        print("\n Prompt:", p)                   # æ‰“å°è¾“å…¥
        print(" Output:", tokenizer.decode(out[0], skip_special_tokens=True))  # è¾“å‡ºç»“æœ

if __name__ == "__main__":                        # ç¨‹åºå…¥å£
    main()                                        # è°ƒç”¨ä¸»å‡½æ•°æ‰§è¡Œ
