import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import torch.nn as nn
import torch.nn.functional as F
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft.tuners.lora.layer import Linear as LoRALinear

# ==========================================
# 1. å¤åˆ¶å¿…è¦çš„æ¨¡å‹å®šä¹‰ (å¿…é¡»ä¸è®­ç»ƒä»£ç ä¸€è‡´)
# ==========================================

class FakeQuantSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale):
        scale = torch.clamp(scale, min=1e-8)
        q = torch.clamp(torch.round(x / scale), -127, 127)
        return q * scale

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output, None

def calc_scale_tokenwise(x, eps=1e-8):
    max_abs = x.detach().abs().amax(dim=-1, keepdim=True)
    return torch.clamp(max_abs / 127.0, min=eps)

class QALoRALayer(nn.Module):
    def __init__(self, base_layer: nn.Module, r: int = 8, lora_alpha: int = 16, group_size: int = 32):
        super().__init__()
        self.in_features = base_layer.in_features
        self.out_features = base_layer.out_features
        self.group_size = group_size
        self.weight = base_layer.weight
        self.r = r
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / r
        self.reduced_dim = self.in_features // group_size
        
        self.lora_A = nn.Parameter(torch.zeros(r, self.reduced_dim))
        self.lora_B = nn.Parameter(torch.zeros(self.out_features, r))
        
        # æ³¨å†Œ buffer ä»¥ä¾¿åŠ è½½æƒé‡
        self.register_buffer('quant_scale', None)
        self.register_buffer('quant_zero', None)

    def fake_quant_activation(self, x):
        scale = calc_scale_tokenwise(x)
        return FakeQuantSTE.apply(x, scale)

    def fake_quant_weight_asym(self, w):
        out_dim, in_dim = w.shape
        w_reshaped = w.reshape(out_dim, in_dim // self.group_size, self.group_size)
        max_val = w_reshaped.amax(dim=-1, keepdim=True)
        min_val = w_reshaped.amin(dim=-1, keepdim=True)
        alpha = (max_val - min_val) / 15.0
        alpha = torch.clamp(alpha, min=1e-5)
        beta = min_val
        w_int = ((w_reshaped - beta) / alpha).round().clamp(0, 15)
        w_recon = w_int * alpha + beta
        w_recon = w_recon.reshape(out_dim, in_dim)
        return w_recon, alpha, beta

    def forward(self, x):
        x_q = self.fake_quant_activation(x)
        w_q, _, _ = self.fake_quant_weight_asym(self.weight)
        base_out = F.linear(x_q, w_q)
        
        b, s, d = x_q.shape
        x_reshaped = x_q.reshape(b, s, d // self.group_size, self.group_size)
        x_pooled = x_reshaped.sum(dim=-1)
        lora_input = x_pooled.to(self.lora_A.dtype)
        lora_out = (lora_input @ self.lora_A.T @ self.lora_B.T) * self.scaling
        
        return base_out + lora_out.to(base_out.dtype)

def convert_to_qalora_structure(model, r=8, group_size=32, target_modules=["q_proj", "v_proj"]):
    """ä»…ç”¨äºæ¨ç†çš„ç»“æ„è½¬æ¢"""
    print(f"ğŸ”„ è½¬æ¢æ¨¡å‹ç»“æ„: r={r}, group_size={group_size}")
    
    def replace_module(module, current_name=""):
        for name, child in module.named_children():
            full_name = f"{current_name}.{name}" if current_name else name
            if isinstance(child, (LoRALinear, nn.Linear)):
                is_target = any(t in name for t in target_modules)
                if isinstance(child, LoRALinear) and is_target:
                    base_layer = child.base_layer
                    new_layer = QALoRALayer(base_layer, r=r, group_size=group_size)
                    new_layer = new_layer.to(base_layer.weight.device).to(base_layer.weight.dtype)
                    setattr(module, name, new_layer)
                elif isinstance(child, nn.Linear) and is_target:
                    new_layer = QALoRALayer(child, r=r, group_size=group_size)
                    new_layer = new_layer.to(child.weight.device).to(child.weight.dtype)
                    setattr(module, name, new_layer)
            else:
                replace_module(child, full_name)

    replace_module(model)
    return model

# ==========================================
# 2. åŠ è½½æ¨¡å‹ä¸é…ç½®
# ==========================================

# --- é…ç½®åŒºåŸŸ ---
MODEL_PATH = "Qwen/Qwen1.5-1.8B-Chat"
# ä½¿ç”¨æ‚¨åˆšåˆšæµ‹è¯•é€šè¿‡çš„ checkpoint è·¯å¾„
CHECKPOINT_PATH = "/root/QALoRA/lora_w4a8_out/r8_g32/stage2_w4a8/checkpoint-600/model.safetensors" 
LORA_RANK = 8
GROUP_SIZE = 32
# ----------------

print(">>> æ­£åœ¨åŠ è½½ Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

print(">>> æ­£åœ¨åŠ è½½ Base Model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH, 
    torch_dtype=torch.float16, 
    device_map="auto", 
    trust_remote_code=True
)

# è½¬æ¢ç»“æ„
model = convert_to_qalora_structure(
    model, 
    r=LORA_RANK, 
    group_size=GROUP_SIZE, 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

print(f">>> æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æƒé‡: {CHECKPOINT_PATH}")
if CHECKPOINT_PATH.endswith(".safetensors"):
    from safetensors.torch import load_file
    state_dict = load_file(CHECKPOINT_PATH)
else:
    state_dict = torch.load(CHECKPOINT_PATH, map_location="cpu")

# åŠ è½½æƒé‡
keys = model.load_state_dict(state_dict, strict=False)
print(f"Load keys info: {keys}")

model.eval()
print(">>> æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ==========================================
# 3. å®šä¹‰æ¨ç†é€»è¾‘
# ==========================================

def predict(text, history=None):
    if history is None:
        history = []
        
    # æ„é€  Prompt (é’ˆå¯¹æƒ…æ„Ÿåˆ†æä»»åŠ¡å¾®è°ƒçš„æ ¼å¼)
    prompt = f"åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼š\nè¯„è®ºï¼š{text}\næƒ…æ„Ÿï¼š"
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=10, 
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            do_sample=False # æƒ…æ„Ÿåˆ†æé€šå¸¸ä¸éœ€è¦é‡‡æ ·
        )
    
    # è§£ç å¹¶æå–ç»“æœ
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå– "æƒ…æ„Ÿï¼š" åé¢çš„éƒ¨åˆ†
    if "æƒ…æ„Ÿï¼š" in full_response:
        response = full_response.split("æƒ…æ„Ÿï¼š")[-1].strip()
    else:
        response = full_response
        
    return response

# ==========================================
# 4. å¯åŠ¨ Web ç•Œé¢ (ç¾åŒ–ç‰ˆ)
# ==========================================

# è‡ªå®šä¹‰ CSS
custom_css = """
.container { max-width: 900px; margin: auto; padding-top: 20px; }
.header-text { text-align: center; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; }
.header-title { font-size: 2.5em; font-weight: bold; color: #2c3e50; margin-bottom: 10px; }
.header-subtitle { font-size: 1.2em; color: #7f8c8d; margin-bottom: 20px; }
.team-info { text-align: center; margin-bottom: 30px; color: #34495e; font-weight: 500; font-size: 1.1em; }
.footer { text-align: center; margin-top: 40px; color: #95a5a6; font-size: 0.8em; }
"""

# ä½¿ç”¨ Soft ä¸»é¢˜
theme = gr.themes.Soft(
    primary_hue="blue",
    secondary_hue="slate",
    neutral_hue="slate",
    font=["sans-serif"]
)

with gr.Blocks(theme=theme, css=custom_css, title="QA-LoRA æƒ…æ„Ÿåˆ†æ") as demo:
    with gr.Column(elem_classes="container"):
        # Header
        gr.Markdown(
            """
            <div class="header-text">
                <div class="header-title">ä¸­æ–‡è¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ</div>
                <div class="header-subtitle">åŸºäº Qwen1.5-1.8B çš„ W4A8 é‡åŒ–å¾®è°ƒæ¨¡å‹</div>
            </div>
            """
        )
        
        # Team Info
        gr.Markdown(
            """
            <div class="team-info">
                ğŸ‘¨â€ğŸ’» å°ç»„æˆå‘˜ï¼š çŸ³æ™¨éœ¡ Â· å´æ˜Šé˜³ Â· å­Ÿä»¤å„’
            </div>
            """
        )

        # Main Content
        with gr.Group():
            with gr.Row():
                with gr.Column(scale=1):
                    input_text = gr.Textbox(
                        label="è¾“å…¥è¯„è®º", 
                        placeholder="è¯·è¾“å…¥æ‚¨æƒ³åˆ†æçš„ä¸­æ–‡è¯„è®º...", 
                        lines=5,
                        show_copy_button=True
                    )
                    
                    with gr.Row():
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                        submit_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary", size="lg")

                with gr.Column(scale=1):
                    output_text = gr.Textbox(
                        label="åˆ†æç»“æœ", 
                        lines=5,
                        interactive=False,
                        show_copy_button=True
                    )
                    
                    # æŠ€æœ¯å‚æ•°æŠ˜å é¢æ¿
                    with gr.Accordion("â„¹ï¸ æ¨¡å‹æŠ€æœ¯å‚æ•°", open=True):
                        gr.Markdown(
                            f"""
                            - **åŸºç¡€æ¨¡å‹**: Qwen/Qwen1.5-1.8B-Chat
                            - **é‡åŒ–æ–¹æ³•**: QA-LoRA (W4A8)
                            - **LoRA Rank**: {LORA_RANK}
                            - **Group Size**: {GROUP_SIZE}
                            - **æ•°æ®é›†**: ChnSentiCorp
                            """
                        )

        # Examples
        gr.Markdown("### ğŸ“ æµ‹è¯•æ ·ä¾‹")
        gr.Examples(
            examples=[
                ["å”è€å¸ˆçš„è¯¾ç¨‹å¤ªæœ‰æ„æ€å•¦ï¼Œå­¦åˆ°äº†å¾ˆå¤šå®ç”¨çš„çŸ¥è¯†ï¼"],
                ["æˆ¿é—´å¾ˆå¹²å‡€ï¼ŒæœåŠ¡ä¹Ÿå¾ˆå‘¨åˆ°ï¼Œä¸‹æ¬¡è¿˜ä¼šæ¥ã€‚"],
                ["éš”éŸ³æ•ˆæœå¤ªå·®äº†ï¼Œä¸€æ™šä¸Šæ²¡ç¡å¥½ã€‚"],
                ["è™½ç„¶ä½ç½®æœ‰ç‚¹åï¼Œä½†æ˜¯æ€§ä»·æ¯”å¾ˆé«˜ã€‚"],
                ["å¿«é€’å¤ªæ…¢äº†ï¼ŒåŒ…è£…ä¹Ÿç ´æŸäº†ï¼Œå·®è¯„ï¼"],
                ["è¿™æœ¬ä¹¦çš„å†…å®¹éå¸¸ç²¾å½©ï¼Œå€¼å¾—ä¸€è¯»ã€‚"]
            ],
            inputs=input_text,
            outputs=output_text,
            fn=predict,
            cache_examples=False,
        )

        # Footer
        # gr.Markdown(
        #     """
        #     <div class="footer">
        #         Powered by QA-LoRA & Gradio | 2025
        #     </div>
        #     """
        # )

    # Event Handlers
    submit_btn.click(fn=predict, inputs=input_text, outputs=output_text)
    clear_btn.click(lambda: ("", ""), outputs=[input_text, output_text])

if __name__ == "__main__":
    # share=True ä¼šç”Ÿæˆä¸€ä¸ªå…¬å…±é“¾æ¥ï¼Œæ–¹ä¾¿æ‚¨åœ¨è¯¾å ‚ä¸Šå±•ç¤º
    # demo.launch(server_name="0.0.0.0", server_port=7860, share=True)
    demo.launch(server_name="0.0.0.0", share=False)
